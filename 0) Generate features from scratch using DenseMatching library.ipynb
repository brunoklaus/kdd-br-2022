{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a1e2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set the correct path to https://github.com/PruneTruong/DenseMatching repository on your filesystem\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "# =======================================================================================++ #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "40ee5d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import imageio.v2 as imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import matplotlib.cm as cm\n",
    "import os.path as os\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from model_selection import model_type, pre_trained_model_types, select_model\n",
    "from datasets.util import pad_to_same_shape\n",
    "torch.set_grad_enabled(False)\n",
    "from utils_flow.pixel_wise_mapping import remap_using_flow_fields\n",
    "from utils_flow.visualization_utils import overlay_semantic_mask, make_sparse_matching_plot\n",
    "from utils_flow.util_optical_flow import flow_to_image  \n",
    "from models.inference_utils import estimate_mask\n",
    "from utils_flow.flow_and_mapping_operations import convert_flow_to_mapping\n",
    "from validation.utils import matches_from_flow\n",
    "from admin.stats import DotDict \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817e64de",
   "metadata": {},
   "source": [
    "# Choose and load the correct model for dense matching. For example, PDCNet+megadepth.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5880e0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose model \n",
    "model = 'PDCNet'\n",
    "pre_trained_model = 'megadepth'\n",
    "flipping_condition = False \n",
    "global_optim_iter = 3\n",
    "local_optim_iter = 7 \n",
    "path_to_pre_trained_models = osp.join(module_path,'assets/pre_trained_models/')\n",
    "    \n",
    "if model not in model_type:\n",
    "    raise ValueError('The model that you chose is not valid: {}'.format(model))\n",
    "if pre_trained_model not in pre_trained_model_types:\n",
    "    raise ValueError('The pre-trained model type that you chose is not valid: {}'.format(pre_trained_model))\n",
    "\n",
    "\n",
    "# inference parameters for PDC-Net\n",
    "network_type = model  # will only use these arguments if the network_type is 'PDCNet' or 'PDCNet_plus'\n",
    "choices_for_multi_stage_types = ['d', 'h', 'ms']\n",
    "multi_stage_type = 'h'\n",
    "if multi_stage_type not in choices_for_multi_stage_types:\n",
    "    raise ValueError('The inference mode that you chose is not valid: {}'.format(multi_stage_type))\n",
    "\n",
    "confidence_map_R =1.0\n",
    "ransac_thresh = 1.0\n",
    "mask_type = 'proba_interval_1_above_10'  # for internal homo estimation\n",
    "homography_visibility_mask = True\n",
    "scaling_factors = [0.5, 0.6, 0.88, 1, 1.33, 1.66, 2]\n",
    "compute_cyclic_consistency_error = True  # here to compare multiple uncertainty \n",
    "\n",
    "# usually from argparse\n",
    "args = DotDict({'network_type': network_type, 'multi_stage_type': multi_stage_type, 'confidence_map_R': confidence_map_R, \n",
    "                'ransac_thresh': ransac_thresh, 'mask_type': mask_type, \n",
    "                'homography_visibility_mask': homography_visibility_mask, 'scaling_factors': scaling_factors, \n",
    "                'compute_cyclic_consistency_error': compute_cyclic_consistency_error})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "906eb099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: PDCNet\n",
      "Pre-trained-model: megadepth\n",
      "GOCor: Local iter 7\n",
      "GOCor: Global iter 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/klaus/anaconda3/envs/dense_matching_env/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/klaus/anaconda3/envs/dense_matching_env/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../assets/pre_trained_models/PDCNet_megadepth.pth.tar\n"
     ]
    }
   ],
   "source": [
    "# define network and load network weights\n",
    "network, estimate_uncertainty = select_model(\n",
    "    model, pre_trained_model, args, global_optim_iter, local_optim_iter,\n",
    "    path_to_pre_trained_models=path_to_pre_trained_models)\n",
    "estimate_uncertainty = True  \n",
    "# here, we overwrite it, to also estimate uncertainty according to forward-backward for networks that do not predict a confidence measure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b381c54c",
   "metadata": {},
   "source": [
    "## Set correct directories for KDD project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2ba6717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found directory with license at /home/klaus/eclipse_draft\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/klaus/eclipse_draft'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "def find_root_folder(starting_path = os.getcwd()):\n",
    "    from os.path import isfile, join, dirname\n",
    "    path = starting_path\n",
    "    i = 0\n",
    "    found = False\n",
    "    while i < 100:\n",
    "        found = ('LICENSE' in [f for f in os.listdir(path) if isfile(join(path, f))])\n",
    "        if found:\n",
    "            break\n",
    "        else:\n",
    "            i += 1\n",
    "            path = dirname(path)            \n",
    "    if found:\n",
    "        print(f\"Found directory with license at {path}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Could not find LICENSE file in ancestral directory\")\n",
    "    return path\n",
    "ROOT_FOLDER = find_root_folder()\n",
    "DS_FOLDER = osp.join(ROOT_FOLDER,'kddbr-2022')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf88dedc",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "071d7256",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 146262/146262 [6:17:02<00:00,  6.47it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "PLOT = False\n",
    "df = pd.read_csv(osp.join(DS_FOLDER,'public.csv'))\n",
    "df['path'] = df['Filename']\n",
    "# We map the filenames on public.csv to the actual path.\n",
    "# to the training image folder\n",
    "df.loc[~pd.isna(df['North']), 'path'] = df.loc[~pd.isna(df['North']), 'path'].apply(lambda x: osp.join(DS_FOLDER, 'train','train', x))\n",
    "# to the test images folder\n",
    "df.loc[pd.isna(df['North']), 'path'] = df.loc[pd.isna(df['North']), 'path'].apply(lambda x: osp.join(DS_FOLDER, 'test','test', x))\n",
    "\n",
    "\n",
    "def get_images(which_id):\n",
    "    \"\"\" Returns the image corresponding to the ``which_id``-th row of the dataframe\"\"\"\n",
    "    img = imageio.imread(df['path'].values[which_id], pilmode='RGB')\n",
    "    query_image, reference_image = img[:,:120,:], img[:,120:,:] \n",
    "    query_image_shape = query_image.shape\n",
    "    ref_image_shape = reference_image.shape\n",
    "    return query_image, reference_image\n",
    "\n",
    "\"\"\"\n",
    "    We create a list L of arrays, each array containing information extracted by the dense matching model.\n",
    "\"\"\"\n",
    "L = []\n",
    "\n",
    "for which_id in trange(df.shape[0]):\n",
    "    query_image, reference_image = get_images(which_id)\n",
    "    \n",
    "    \"\"\" \n",
    "        Prepare query and reference images. \n",
    "    \"\"\"\n",
    "    # convert the images to correct format to be processed by the network: torch Tensors, format B, C, H, W. \n",
    "    # pad both images to the same size, to be processed by network\n",
    "    query_image_, reference_image_ = pad_to_same_shape(query_image, reference_image)\n",
    "\n",
    "    # convert numpy to torch tensor and put it in right format\n",
    "    query_image_ = torch.from_numpy(query_image_).permute(2, 0, 1).unsqueeze(0)\n",
    "    reference_image_ = torch.from_numpy(reference_image_).permute(2, 0, 1).unsqueeze(0)\n",
    "    \"\"\" \n",
    "        Estimate flow / uncertainty using the given model. \n",
    "    \"\"\"\n",
    "    if estimate_uncertainty:\n",
    "        estimated_flow, uncertainty_components = network.estimate_flow_and_confidence_map(query_image_, reference_image_)\n",
    "    else:\n",
    "        if args.flipping_condition and 'GLUNet' in args.model:\n",
    "            estimated_flow = network.estimate_flow_with_flipping_condition(query_image_, reference_image_,\n",
    "                                                                           mode='channel_first')\n",
    "        else:\n",
    "            estimated_flow = network.estimate_flow(query_image_, reference_image_, mode='channel_first')\n",
    "    # removes the padding\n",
    "    estimated_flow = estimated_flow[:, :, :ref_image_shape[0], :ref_image_shape[1]]\n",
    "\n",
    "    # convert to numpy and reformat\n",
    "    estimated_flow_numpy = estimated_flow.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    # warp the query image according to the estimated flow\n",
    "    warped_query_image = remap_using_flow_fields(query_image, estimated_flow_numpy[:, :, 0],\n",
    "                                                 estimated_flow_numpy[:, :, 1]).astype(np.uint8)\n",
    "    alpha = 0.5\n",
    "    img_warped_overlay_on_target_masked = warped_query_image * alpha + reference_image * alpha\n",
    "    # confidence estimation + visualization\n",
    "    if not estimate_uncertainty: \n",
    "        raise ValueError\n",
    "    uncertainty_key = 'p_r'  # 'inv_cyclic_consistency_error' \n",
    "    #'p_r', 'inv_cyclic_consistency_error' can also be used as a confidence measure\n",
    "    # 'cyclic_consistency_error' can also be used, but that's an uncertainty measure\n",
    "    min_confidence = 0.30\n",
    "    confidence_map = uncertainty_components[uncertainty_key]\n",
    "    confidence_map = confidence_map[:, :, :ref_image_shape[0], :ref_image_shape[1]]\n",
    "    confidence_map_numpy = confidence_map.squeeze().detach().cpu().numpy()\n",
    "    \n",
    "    color = [255, 102, 51]\n",
    "    confidence_map_numpy = confidence_map.squeeze().detach().cpu().numpy()\n",
    "    confident_mask = (confidence_map_numpy > min_confidence).astype(np.uint8)\n",
    "    confident_warped = overlay_semantic_mask(warped_query_image, ann=255 - confident_mask*255, color=color)\n",
    "    # get the mask according to uncertainty estimation\n",
    "    mask_type = 'proba_interval_1_above_10' # 'cyclic_consistency_error_below_2' \n",
    "    mask_padded = estimate_mask(mask_type, uncertainty_components) \n",
    "    if 'warping_mask' in list(uncertainty_components.keys()):\n",
    "        # get mask from internal multi stage alignment, if it took place\n",
    "        mask_padded = uncertainty_components['warping_mask'] * mask_padded\n",
    "    # remove the padding\n",
    "    mask = mask_padded[:, :ref_image_shape[0], :ref_image_shape[1]]\n",
    "    # remove point that lead to outside the query image\n",
    "    mapping_estimated = convert_flow_to_mapping(estimated_flow)\n",
    "    mask = mask & mapping_estimated[:, 0].ge(0) & mapping_estimated[:, 1].ge(0) & \\\n",
    "    mapping_estimated[:, 0].le(query_image_shape[1] - 1) & mapping_estimated[:, 1].le(query_image_shape[0] - 1)\n",
    "\n",
    "    mkpts_query, mkpts_ref = matches_from_flow(estimated_flow, mask)\n",
    "\n",
    "    confidence_values = confidence_map.squeeze()[mask.squeeze()].cpu().numpy()\n",
    "    sort_index = np.argsort(np.array(confidence_values)).tolist()[::-1]  # from highest to smallest\n",
    "    confidence_values = np.array(confidence_values)[sort_index]\n",
    "    mkpts_query = np.array(mkpts_query)[sort_index]\n",
    "    mkpts_ref = np.array(mkpts_ref)[sort_index]\n",
    "\n",
    "    if len(mkpts_query) > 5:\n",
    "        \"\"\" Use OpenCV to find the best homography transformation,\n",
    "            given the marker points obtained from dense matching.\n",
    "        \"\"\"\n",
    "        M, mask = cv.findHomography(mkpts_query, mkpts_ref, cv.RANSAC,5.0)\n",
    "    else:\n",
    "        M = np.zeros((3,3))\n",
    "    \n",
    "    \"\"\" Create feature vector.\"\"\"\n",
    "    confidence_map_numpy = confidence_map_numpy.reshape(-1)\n",
    "    res = np.zeros(20)\n",
    "    res[:9] = M.reshape(-1)\n",
    "    res[9] = len(mkpts_query)\n",
    "    res[10] = confidence_map_numpy.reshape(-1).mean()\n",
    "    res[11] = estimated_flow_numpy[:,:,0].mean()\n",
    "    res[12] = estimated_flow_numpy[:,:,0].std()\n",
    "    res[13] = estimated_flow_numpy[:,:,1].mean()\n",
    "    res[14] = estimated_flow_numpy[:,:,1].std()\n",
    "    res[15:20] = np.quantile(confidence_map_numpy,[0,0.1,0.25,0.5,1])\n",
    "    \n",
    "    # Append feature vector to list\n",
    "    L.append(res[None,:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422f92a2",
   "metadata": {},
   "source": [
    "## We concatenate all the arrays from the list L into a big array X. From it, we create a feature dataframe and save it for (training + validation) and test subsets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "34823cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146262\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mat_0</th>\n",
       "      <th>mat_1</th>\n",
       "      <th>mat_2</th>\n",
       "      <th>mat_3</th>\n",
       "      <th>mat_4</th>\n",
       "      <th>mat_5</th>\n",
       "      <th>mat_6</th>\n",
       "      <th>mat_7</th>\n",
       "      <th>mat_8</th>\n",
       "      <th>n_pts</th>\n",
       "      <th>cm_mean</th>\n",
       "      <th>ef0_mean</th>\n",
       "      <th>ef0_std</th>\n",
       "      <th>ef1_mean</th>\n",
       "      <th>ef1_std</th>\n",
       "      <th>cm_q0</th>\n",
       "      <th>cm_q1</th>\n",
       "      <th>cm_q2</th>\n",
       "      <th>cm_q3</th>\n",
       "      <th>cm_q4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.002423</td>\n",
       "      <td>-0.001384</td>\n",
       "      <td>-2.929688</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>1.001043</td>\n",
       "      <td>-1.337759</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>4.164572e-06</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13764.0</td>\n",
       "      <td>0.572765</td>\n",
       "      <td>2.888732</td>\n",
       "      <td>0.118693</td>\n",
       "      <td>1.277729</td>\n",
       "      <td>0.083238</td>\n",
       "      <td>0.464305</td>\n",
       "      <td>0.572845</td>\n",
       "      <td>0.572869</td>\n",
       "      <td>0.572871</td>\n",
       "      <td>0.572872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.998573</td>\n",
       "      <td>0.001190</td>\n",
       "      <td>-0.353788</td>\n",
       "      <td>0.001521</td>\n",
       "      <td>0.997956</td>\n",
       "      <td>0.848945</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>4.255099e-07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14096.0</td>\n",
       "      <td>0.567747</td>\n",
       "      <td>0.262010</td>\n",
       "      <td>0.151464</td>\n",
       "      <td>-0.902149</td>\n",
       "      <td>0.223343</td>\n",
       "      <td>0.174298</td>\n",
       "      <td>0.568731</td>\n",
       "      <td>0.572271</td>\n",
       "      <td>0.572810</td>\n",
       "      <td>0.572872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.996697</td>\n",
       "      <td>0.001945</td>\n",
       "      <td>1.621848</td>\n",
       "      <td>-0.002061</td>\n",
       "      <td>1.000195</td>\n",
       "      <td>-3.803263</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>2.240335e-05</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13578.0</td>\n",
       "      <td>0.572625</td>\n",
       "      <td>-1.623801</td>\n",
       "      <td>0.073526</td>\n",
       "      <td>3.904920</td>\n",
       "      <td>0.115052</td>\n",
       "      <td>0.390947</td>\n",
       "      <td>0.572856</td>\n",
       "      <td>0.572867</td>\n",
       "      <td>0.572870</td>\n",
       "      <td>0.572872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.981403</td>\n",
       "      <td>-0.010081</td>\n",
       "      <td>3.957249</td>\n",
       "      <td>0.010763</td>\n",
       "      <td>0.995305</td>\n",
       "      <td>-6.589612</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>-6.674789e-05</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13107.0</td>\n",
       "      <td>0.572373</td>\n",
       "      <td>-2.456800</td>\n",
       "      <td>0.589099</td>\n",
       "      <td>5.979247</td>\n",
       "      <td>0.431804</td>\n",
       "      <td>0.453649</td>\n",
       "      <td>0.572844</td>\n",
       "      <td>0.572870</td>\n",
       "      <td>0.572872</td>\n",
       "      <td>0.572872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.003583</td>\n",
       "      <td>0.002621</td>\n",
       "      <td>-8.319884</td>\n",
       "      <td>0.001843</td>\n",
       "      <td>1.001703</td>\n",
       "      <td>-0.813738</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>-3.694162e-05</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13196.0</td>\n",
       "      <td>0.571685</td>\n",
       "      <td>8.009210</td>\n",
       "      <td>0.303784</td>\n",
       "      <td>0.571931</td>\n",
       "      <td>0.147131</td>\n",
       "      <td>0.362815</td>\n",
       "      <td>0.572329</td>\n",
       "      <td>0.572865</td>\n",
       "      <td>0.572870</td>\n",
       "      <td>0.572872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146257</th>\n",
       "      <td>1.037009</td>\n",
       "      <td>0.026269</td>\n",
       "      <td>-10.391671</td>\n",
       "      <td>-0.013865</td>\n",
       "      <td>0.993531</td>\n",
       "      <td>10.232949</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>6.216618e-05</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12482.0</td>\n",
       "      <td>0.571941</td>\n",
       "      <td>7.084321</td>\n",
       "      <td>1.155725</td>\n",
       "      <td>-8.482187</td>\n",
       "      <td>0.845956</td>\n",
       "      <td>0.343910</td>\n",
       "      <td>0.572322</td>\n",
       "      <td>0.572844</td>\n",
       "      <td>0.572868</td>\n",
       "      <td>0.572872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146258</th>\n",
       "      <td>1.009781</td>\n",
       "      <td>-0.007256</td>\n",
       "      <td>-11.139119</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>0.999674</td>\n",
       "      <td>-0.489926</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>8.783195e-05</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12732.0</td>\n",
       "      <td>0.572101</td>\n",
       "      <td>11.016906</td>\n",
       "      <td>0.601743</td>\n",
       "      <td>0.772840</td>\n",
       "      <td>0.306503</td>\n",
       "      <td>0.489332</td>\n",
       "      <td>0.572452</td>\n",
       "      <td>0.572865</td>\n",
       "      <td>0.572870</td>\n",
       "      <td>0.572872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146259</th>\n",
       "      <td>1.004349</td>\n",
       "      <td>-0.005489</td>\n",
       "      <td>-2.613301</td>\n",
       "      <td>0.000747</td>\n",
       "      <td>0.998082</td>\n",
       "      <td>3.039214</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>-5.560081e-05</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13451.0</td>\n",
       "      <td>0.572544</td>\n",
       "      <td>2.684564</td>\n",
       "      <td>0.147633</td>\n",
       "      <td>-3.073310</td>\n",
       "      <td>0.159691</td>\n",
       "      <td>0.330920</td>\n",
       "      <td>0.572717</td>\n",
       "      <td>0.572862</td>\n",
       "      <td>0.572869</td>\n",
       "      <td>0.572872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146260</th>\n",
       "      <td>0.986190</td>\n",
       "      <td>0.000395</td>\n",
       "      <td>7.049865</td>\n",
       "      <td>0.003211</td>\n",
       "      <td>0.997465</td>\n",
       "      <td>-2.680462</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>2.011234e-06</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13031.0</td>\n",
       "      <td>0.572071</td>\n",
       "      <td>-6.440472</td>\n",
       "      <td>0.389240</td>\n",
       "      <td>2.600944</td>\n",
       "      <td>0.188957</td>\n",
       "      <td>0.385985</td>\n",
       "      <td>0.572845</td>\n",
       "      <td>0.572867</td>\n",
       "      <td>0.572871</td>\n",
       "      <td>0.572872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146261</th>\n",
       "      <td>0.996758</td>\n",
       "      <td>0.099603</td>\n",
       "      <td>-18.575682</td>\n",
       "      <td>-0.091991</td>\n",
       "      <td>0.957461</td>\n",
       "      <td>17.245772</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>-2.826846e-04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11514.0</td>\n",
       "      <td>0.563716</td>\n",
       "      <td>13.909675</td>\n",
       "      <td>4.174212</td>\n",
       "      <td>-8.559829</td>\n",
       "      <td>3.846148</td>\n",
       "      <td>0.050530</td>\n",
       "      <td>0.560388</td>\n",
       "      <td>0.572127</td>\n",
       "      <td>0.572849</td>\n",
       "      <td>0.572872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>146262 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           mat_0     mat_1      mat_2     mat_3     mat_4      mat_5  \\\n",
       "0       1.002423 -0.001384  -2.929688  0.000169  1.001043  -1.337759   \n",
       "1       0.998573  0.001190  -0.353788  0.001521  0.997956   0.848945   \n",
       "2       0.996697  0.001945   1.621848 -0.002061  1.000195  -3.803263   \n",
       "3       0.981403 -0.010081   3.957249  0.010763  0.995305  -6.589612   \n",
       "4       1.003583  0.002621  -8.319884  0.001843  1.001703  -0.813738   \n",
       "...          ...       ...        ...       ...       ...        ...   \n",
       "146257  1.037009  0.026269 -10.391671 -0.013865  0.993531  10.232949   \n",
       "146258  1.009781 -0.007256 -11.139119  0.000272  0.999674  -0.489926   \n",
       "146259  1.004349 -0.005489  -2.613301  0.000747  0.998082   3.039214   \n",
       "146260  0.986190  0.000395   7.049865  0.003211  0.997465  -2.680462   \n",
       "146261  0.996758  0.099603 -18.575682 -0.091991  0.957461  17.245772   \n",
       "\n",
       "           mat_6         mat_7  mat_8    n_pts   cm_mean   ef0_mean   ef0_std  \\\n",
       "0       0.000001  4.164572e-06    1.0  13764.0  0.572765   2.888732  0.118693   \n",
       "1      -0.000023  4.255099e-07    1.0  14096.0  0.567747   0.262010  0.151464   \n",
       "2      -0.000034  2.240335e-05    1.0  13578.0  0.572625  -1.623801  0.073526   \n",
       "3       0.000009 -6.674789e-05    1.0  13107.0  0.572373  -2.456800  0.589099   \n",
       "4       0.000041 -3.694162e-05    1.0  13196.0  0.571685   8.009210  0.303784   \n",
       "...          ...           ...    ...      ...       ...        ...       ...   \n",
       "146257  0.000061  6.216618e-05    1.0  12482.0  0.571941   7.084321  1.155725   \n",
       "146258 -0.000030  8.783195e-05    1.0  12732.0  0.572101  11.016906  0.601743   \n",
       "146259  0.000044 -5.560081e-05    1.0  13451.0  0.572544   2.684564  0.147633   \n",
       "146260 -0.000026  2.011234e-06    1.0  13031.0  0.572071  -6.440472  0.389240   \n",
       "146261  0.000211 -2.826846e-04    1.0  11514.0  0.563716  13.909675  4.174212   \n",
       "\n",
       "        ef1_mean   ef1_std     cm_q0     cm_q1     cm_q2     cm_q3     cm_q4  \n",
       "0       1.277729  0.083238  0.464305  0.572845  0.572869  0.572871  0.572872  \n",
       "1      -0.902149  0.223343  0.174298  0.568731  0.572271  0.572810  0.572872  \n",
       "2       3.904920  0.115052  0.390947  0.572856  0.572867  0.572870  0.572872  \n",
       "3       5.979247  0.431804  0.453649  0.572844  0.572870  0.572872  0.572872  \n",
       "4       0.571931  0.147131  0.362815  0.572329  0.572865  0.572870  0.572872  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "146257 -8.482187  0.845956  0.343910  0.572322  0.572844  0.572868  0.572872  \n",
       "146258  0.772840  0.306503  0.489332  0.572452  0.572865  0.572870  0.572872  \n",
       "146259 -3.073310  0.159691  0.330920  0.572717  0.572862  0.572869  0.572872  \n",
       "146260  2.600944  0.188957  0.385985  0.572845  0.572867  0.572871  0.572872  \n",
       "146261 -8.559829  3.846148  0.050530  0.560388  0.572127  0.572849  0.572872  \n",
       "\n",
       "[146262 rows x 20 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "X = np.concatenate(L,axis=0)\n",
    "col = [f'mat_{i}' for i in range(9)] + ['n_pts'] + ['cm_mean','ef0_mean','ef0_std','ef1_mean','ef1_std'] +  [f'cm_q{i}' for i in range(5)]\n",
    "feat_df = pd.DataFrame(X,index=range(X.shape[0]),columns=col)\n",
    "display(feat_df)\n",
    "\n",
    "# Define and create output folder if it does not exist\n",
    "OUT_FOLDER  = osp.join(ROOT_FOLDER,'features', model + '-' + pre_trained_model) \n",
    "os.makedirs(OUT_FOLDER, exist_ok  = True)\n",
    "\n",
    "feat_df.loc[~pd.isna(df['North']), :].to_csv(osp.join(OUT_FOLDER,'trainval.csv'),index=False)\n",
    "feat_df.loc[pd.isna(df['North']), :].to_csv(osp.join(OUT_FOLDER,'test.csv'),index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed98bc53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
