{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e535e213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set the correct path to https://github.com/PruneTruong/DenseMatching repository on your filesystem\n",
    "module_path = \"/home/klaus/eclipse_draft/DenseMatching/DenseMatching/\"\n",
    "# =======================================================================================++ #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40ee5d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/klaus/anaconda3/envs/dense_matching_env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not load moviepy\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import imageio.v2 as imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import matplotlib.cm as cm\n",
    "import os.path as os\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from model_selection import model_type, pre_trained_model_types, select_model\n",
    "from datasets.util import pad_to_same_shape\n",
    "torch.set_grad_enabled(False)\n",
    "from utils_flow.pixel_wise_mapping import remap_using_flow_fields\n",
    "from utils_flow.visualization_utils import overlay_semantic_mask, make_sparse_matching_plot\n",
    "from utils_flow.util_optical_flow import flow_to_image  \n",
    "from models.inference_utils import estimate_mask\n",
    "from utils_flow.flow_and_mapping_operations import convert_flow_to_mapping\n",
    "from validation.utils import matches_from_flow\n",
    "from admin.stats import DotDict \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817e64de",
   "metadata": {},
   "source": [
    "# Define the images to match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5880e0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "\n",
    "# choose model \n",
    "model = 'PDCNet'\n",
    "pre_trained_model = 'megadepth'\n",
    "flipping_condition = False \n",
    "global_optim_iter = 3\n",
    "local_optim_iter = 7 \n",
    "path_to_pre_trained_models = osp.join(module_path,'assets/pre_trained_models/')\n",
    "    \n",
    "if model not in model_type:\n",
    "    raise ValueError('The model that you chose is not valid: {}'.format(model))\n",
    "if pre_trained_model not in pre_trained_model_types:\n",
    "    raise ValueError('The pre-trained model type that you chose is not valid: {}'.format(pre_trained_model))\n",
    "\n",
    "\n",
    "# inference parameters for PDC-Net\n",
    "network_type = model  # will only use these arguments if the network_type is 'PDCNet' or 'PDCNet_plus'\n",
    "choices_for_multi_stage_types = ['d', 'h', 'ms']\n",
    "multi_stage_type = 'h'\n",
    "if multi_stage_type not in choices_for_multi_stage_types:\n",
    "    raise ValueError('The inference mode that you chose is not valid: {}'.format(multi_stage_type))\n",
    "\n",
    "confidence_map_R =1.0\n",
    "ransac_thresh = 1.0\n",
    "mask_type = 'proba_interval_1_above_10'  # for internal homo estimation\n",
    "homography_visibility_mask = True\n",
    "scaling_factors = [0.5, 0.6, 0.88, 1, 1.33, 1.66, 2]\n",
    "compute_cyclic_consistency_error = True  # here to compare multiple uncertainty \n",
    "\n",
    "# usually from argparse\n",
    "args = DotDict({'network_type': network_type, 'multi_stage_type': multi_stage_type, 'confidence_map_R': confidence_map_R, \n",
    "                'ransac_thresh': ransac_thresh, 'mask_type': mask_type, \n",
    "                'homography_visibility_mask': homography_visibility_mask, 'scaling_factors': scaling_factors, \n",
    "                'compute_cyclic_consistency_error': compute_cyclic_consistency_error})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "906eb099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: PDCNet\n",
      "Pre-trained-model: megadepth\n",
      "GOCor: Local iter 7\n",
      "GOCor: Global iter 3\n",
      "/home/klaus/eclipse_draft/DenseMatching/DenseMatching/assets/pre_trained_models/PDCNet_megadepth.pth.tar\n"
     ]
    }
   ],
   "source": [
    "# define network and load network weights\n",
    "network, estimate_uncertainty = select_model(\n",
    "    model, pre_trained_model, args, global_optim_iter, local_optim_iter,\n",
    "    path_to_pre_trained_models=path_to_pre_trained_models)\n",
    "estimate_uncertainty = True  \n",
    "# here, we overwrite it, to also estimate uncertainty according to forward-backward for networks that do not predict a confidence measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "071d7256",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:01<00:00,  6.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mat_0</th>\n",
       "      <th>mat_1</th>\n",
       "      <th>mat_2</th>\n",
       "      <th>mat_3</th>\n",
       "      <th>mat_4</th>\n",
       "      <th>mat_5</th>\n",
       "      <th>mat_6</th>\n",
       "      <th>mat_7</th>\n",
       "      <th>mat_8</th>\n",
       "      <th>ef0_0</th>\n",
       "      <th>...</th>\n",
       "      <th>ef1_6</th>\n",
       "      <th>ef1_7</th>\n",
       "      <th>ef1_8</th>\n",
       "      <th>ef1_9</th>\n",
       "      <th>ef1_10</th>\n",
       "      <th>conf_0</th>\n",
       "      <th>conf_1</th>\n",
       "      <th>conf_2</th>\n",
       "      <th>conf_3</th>\n",
       "      <th>conf_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.997199</td>\n",
       "      <td>0.001646</td>\n",
       "      <td>2.977600</td>\n",
       "      <td>-0.000095</td>\n",
       "      <td>0.998869</td>\n",
       "      <td>1.308397</td>\n",
       "      <td>-5.093368e-06</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.067490</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.250218</td>\n",
       "      <td>-1.242380</td>\n",
       "      <td>-1.231876</td>\n",
       "      <td>-1.209067</td>\n",
       "      <td>-0.735909</td>\n",
       "      <td>0.300139</td>\n",
       "      <td>0.572847</td>\n",
       "      <td>0.572869</td>\n",
       "      <td>0.572871</td>\n",
       "      <td>0.572872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.005845</td>\n",
       "      <td>0.000628</td>\n",
       "      <td>0.191164</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>1.005789</td>\n",
       "      <td>-1.129162</td>\n",
       "      <td>6.401204e-05</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.495178</td>\n",
       "      <td>...</td>\n",
       "      <td>0.917903</td>\n",
       "      <td>0.959494</td>\n",
       "      <td>1.018648</td>\n",
       "      <td>1.114135</td>\n",
       "      <td>2.080652</td>\n",
       "      <td>0.295980</td>\n",
       "      <td>0.563280</td>\n",
       "      <td>0.571988</td>\n",
       "      <td>0.572827</td>\n",
       "      <td>0.572872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.002666</td>\n",
       "      <td>-0.000808</td>\n",
       "      <td>-1.642501</td>\n",
       "      <td>0.001741</td>\n",
       "      <td>1.000746</td>\n",
       "      <td>3.786195</td>\n",
       "      <td>2.257165e-05</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.120789</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.890309</td>\n",
       "      <td>-3.870249</td>\n",
       "      <td>-3.844637</td>\n",
       "      <td>-3.803320</td>\n",
       "      <td>-3.251992</td>\n",
       "      <td>0.462080</td>\n",
       "      <td>0.572859</td>\n",
       "      <td>0.572868</td>\n",
       "      <td>0.572871</td>\n",
       "      <td>0.572872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.020131</td>\n",
       "      <td>0.010518</td>\n",
       "      <td>-3.981294</td>\n",
       "      <td>-0.010265</td>\n",
       "      <td>1.006860</td>\n",
       "      <td>6.580093</td>\n",
       "      <td>2.453464e-06</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.244383</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.783880</td>\n",
       "      <td>-5.661088</td>\n",
       "      <td>-5.581955</td>\n",
       "      <td>-5.453252</td>\n",
       "      <td>-4.195916</td>\n",
       "      <td>0.391697</td>\n",
       "      <td>0.572859</td>\n",
       "      <td>0.572870</td>\n",
       "      <td>0.572872</td>\n",
       "      <td>0.572872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.996602</td>\n",
       "      <td>-0.003134</td>\n",
       "      <td>8.360514</td>\n",
       "      <td>-0.001560</td>\n",
       "      <td>0.998482</td>\n",
       "      <td>0.758233</td>\n",
       "      <td>-3.319132e-05</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.995064</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.533055</td>\n",
       "      <td>-0.499078</td>\n",
       "      <td>-0.467451</td>\n",
       "      <td>-0.436842</td>\n",
       "      <td>-0.106895</td>\n",
       "      <td>0.237026</td>\n",
       "      <td>0.572477</td>\n",
       "      <td>0.572865</td>\n",
       "      <td>0.572870</td>\n",
       "      <td>0.572872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.000646</td>\n",
       "      <td>-0.001438</td>\n",
       "      <td>-0.207467</td>\n",
       "      <td>-0.000240</td>\n",
       "      <td>0.999420</td>\n",
       "      <td>1.297872</td>\n",
       "      <td>9.990387e-06</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.234032</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.254985</td>\n",
       "      <td>-1.242882</td>\n",
       "      <td>-1.228972</td>\n",
       "      <td>-1.206512</td>\n",
       "      <td>-0.841086</td>\n",
       "      <td>0.492234</td>\n",
       "      <td>0.572856</td>\n",
       "      <td>0.572867</td>\n",
       "      <td>0.572870</td>\n",
       "      <td>0.572872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.005041</td>\n",
       "      <td>0.001884</td>\n",
       "      <td>-5.048995</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>1.005335</td>\n",
       "      <td>-1.999054</td>\n",
       "      <td>5.190567e-06</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.086952</td>\n",
       "      <td>...</td>\n",
       "      <td>1.833006</td>\n",
       "      <td>1.853075</td>\n",
       "      <td>1.875781</td>\n",
       "      <td>1.935270</td>\n",
       "      <td>3.170885</td>\n",
       "      <td>0.446535</td>\n",
       "      <td>0.572225</td>\n",
       "      <td>0.572825</td>\n",
       "      <td>0.572866</td>\n",
       "      <td>0.572872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.987696</td>\n",
       "      <td>-0.023689</td>\n",
       "      <td>1.705560</td>\n",
       "      <td>0.011023</td>\n",
       "      <td>0.977183</td>\n",
       "      <td>6.166472</td>\n",
       "      <td>1.872593e-08</td>\n",
       "      <td>-0.000192</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.536316</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.330072</td>\n",
       "      <td>-6.189418</td>\n",
       "      <td>-6.047063</td>\n",
       "      <td>-5.847726</td>\n",
       "      <td>-5.330689</td>\n",
       "      <td>0.324774</td>\n",
       "      <td>0.572826</td>\n",
       "      <td>0.572863</td>\n",
       "      <td>0.572869</td>\n",
       "      <td>0.572872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.947952</td>\n",
       "      <td>0.002231</td>\n",
       "      <td>10.153619</td>\n",
       "      <td>-0.039127</td>\n",
       "      <td>0.989292</td>\n",
       "      <td>9.931049</td>\n",
       "      <td>-2.589543e-04</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-10.984010</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.456038</td>\n",
       "      <td>-7.400121</td>\n",
       "      <td>-7.338771</td>\n",
       "      <td>-7.265960</td>\n",
       "      <td>-6.524284</td>\n",
       "      <td>0.103602</td>\n",
       "      <td>0.555731</td>\n",
       "      <td>0.572159</td>\n",
       "      <td>0.572844</td>\n",
       "      <td>0.572872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.000098</td>\n",
       "      <td>-0.003899</td>\n",
       "      <td>-2.332422</td>\n",
       "      <td>-0.000402</td>\n",
       "      <td>1.000340</td>\n",
       "      <td>-6.612254</td>\n",
       "      <td>-2.508822e-05</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.397377</td>\n",
       "      <td>...</td>\n",
       "      <td>6.321239</td>\n",
       "      <td>6.389812</td>\n",
       "      <td>6.494902</td>\n",
       "      <td>6.707514</td>\n",
       "      <td>8.986298</td>\n",
       "      <td>0.203598</td>\n",
       "      <td>0.565570</td>\n",
       "      <td>0.572506</td>\n",
       "      <td>0.572865</td>\n",
       "      <td>0.572872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      mat_0     mat_1      mat_2     mat_3     mat_4     mat_5         mat_6  \\\n",
       "0  0.997199  0.001646   2.977600 -0.000095  0.998869  1.308397 -5.093368e-06   \n",
       "1  1.005845  0.000628   0.191164  0.002976  1.005789 -1.129162  6.401204e-05   \n",
       "2  1.002666 -0.000808  -1.642501  0.001741  1.000746  3.786195  2.257165e-05   \n",
       "3  1.020131  0.010518  -3.981294 -0.010265  1.006860  6.580093  2.453464e-06   \n",
       "4  0.996602 -0.003134   8.360514 -0.001560  0.998482  0.758233 -3.319132e-05   \n",
       "5  1.000646 -0.001438  -0.207467 -0.000240  0.999420  1.297872  9.990387e-06   \n",
       "6  1.005041  0.001884  -5.048995  0.000024  1.005335 -1.999054  5.190567e-06   \n",
       "7  0.987696 -0.023689   1.705560  0.011023  0.977183  6.166472  1.872593e-08   \n",
       "8  0.947952  0.002231  10.153619 -0.039127  0.989292  9.931049 -2.589543e-04   \n",
       "9  1.000098 -0.003899  -2.332422 -0.000402  1.000340 -6.612254 -2.508822e-05   \n",
       "\n",
       "      mat_7  mat_8      ef0_0  ...     ef1_6     ef1_7     ef1_8     ef1_9  \\\n",
       "0 -0.000002    1.0  -5.067490  ... -1.250218 -1.242380 -1.231876 -1.209067   \n",
       "1  0.000004    1.0  -1.495178  ...  0.917903  0.959494  1.018648  1.114135   \n",
       "2 -0.000010    1.0   0.120789  ... -3.890309 -3.870249 -3.844637 -3.803320   \n",
       "3  0.000079    1.0   1.244383  ... -5.783880 -5.661088 -5.581955 -5.453252   \n",
       "4  0.000026    1.0  -9.995064  ... -0.533055 -0.499078 -0.467451 -0.436842   \n",
       "5 -0.000015    1.0  -0.234032  ... -1.254985 -1.242882 -1.228972 -1.206512   \n",
       "6  0.000027    1.0   4.086952  ...  1.833006  1.853075  1.875781  1.935270   \n",
       "7 -0.000192    1.0  -2.536316  ... -6.330072 -6.189418 -6.047063 -5.847726   \n",
       "8  0.000008    1.0 -10.984010  ... -7.456038 -7.400121 -7.338771 -7.265960   \n",
       "9 -0.000070    1.0   1.397377  ...  6.321239  6.389812  6.494902  6.707514   \n",
       "\n",
       "     ef1_10    conf_0    conf_1    conf_2    conf_3    conf_4  \n",
       "0 -0.735909  0.300139  0.572847  0.572869  0.572871  0.572872  \n",
       "1  2.080652  0.295980  0.563280  0.571988  0.572827  0.572872  \n",
       "2 -3.251992  0.462080  0.572859  0.572868  0.572871  0.572872  \n",
       "3 -4.195916  0.391697  0.572859  0.572870  0.572872  0.572872  \n",
       "4 -0.106895  0.237026  0.572477  0.572865  0.572870  0.572872  \n",
       "5 -0.841086  0.492234  0.572856  0.572867  0.572870  0.572872  \n",
       "6  3.170885  0.446535  0.572225  0.572825  0.572866  0.572872  \n",
       "7 -5.330689  0.324774  0.572826  0.572863  0.572869  0.572872  \n",
       "8 -6.524284  0.103602  0.555731  0.572159  0.572844  0.572872  \n",
       "9  8.986298  0.203598  0.565570  0.572506  0.572865  0.572872  \n",
       "\n",
       "[10 rows x 36 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "exceptions must derive from BaseException",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 143>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    141\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(osp\u001b[38;5;241m.\u001b[39mjoin(DS_FOLDER,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m,model),exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    142\u001b[0m display(feat_df)\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    145\u001b[0m feat_df\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;241m~\u001b[39mpd\u001b[38;5;241m.\u001b[39misna(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNorth\u001b[39m\u001b[38;5;124m'\u001b[39m]), :]\u001b[38;5;241m.\u001b[39mto_csv(osp\u001b[38;5;241m.\u001b[39mjoin(DS_FOLDER,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m,model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrainval.csv\u001b[39m\u001b[38;5;124m'\u001b[39m),index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    146\u001b[0m feat_df\u001b[38;5;241m.\u001b[39mloc[pd\u001b[38;5;241m.\u001b[39misna(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNorth\u001b[39m\u001b[38;5;124m'\u001b[39m]), :]\u001b[38;5;241m.\u001b[39mto_csv(osp\u001b[38;5;241m.\u001b[39mjoin(DS_FOLDER,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m,model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest.csv\u001b[39m\u001b[38;5;124m'\u001b[39m),index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: exceptions must derive from BaseException"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "import cv2 as cv\n",
    "from tqdm import tqdm, trange\n",
    "DS_FOLDER = '/home/klaus/eclipse_draft/KDD2022/kddbr-2022/'\n",
    "PLOT = False # Used for debugging.\n",
    "df = pd.read_csv(osp.join(DS_FOLDER,'public.csv'))\n",
    "df['path'] = df['Filename']\n",
    "df.loc[~pd.isna(df['North']), 'path'] = df.loc[~pd.isna(df['North']), 'path'].apply(lambda x: osp.join(DS_FOLDER, 'train','train', x))\n",
    "df.loc[pd.isna(df['North']), 'path'] = df.loc[pd.isna(df['North']), 'path'].apply(lambda x: osp.join(DS_FOLDER, 'test','test', x))\n",
    "\n",
    "\n",
    "def get_images(which_id):\n",
    "    img = imageio.imread(df['path'].values[which_id], pilmode='RGB')\n",
    "    query_image, reference_image = img[:,:120,:], img[:,120:,:] \n",
    "    query_image_shape = query_image.shape\n",
    "    ref_image_shape = reference_image.shape\n",
    "    if PLOT:\n",
    "        fig, axis = plt.subplots(2, figsize=(10,5))\n",
    "        axis[0].imshow(query_image)\n",
    "        axis[0].set_title('Query image')\n",
    "        axis[1].imshow(reference_image)\n",
    "        axis[1].set_title('Reference image')\n",
    "        plt.show()\n",
    "    return query_image, reference_image\n",
    "\n",
    "\n",
    "L = []\n",
    "\n",
    "for which_id in trange(df.shape[0]):\n",
    "    \n",
    "    # INVERT!\n",
    "    reference_image, query_image = get_images(which_id)\n",
    "    \n",
    "    fname = df['Filename'].values[which_id]\n",
    "    alt = df['Altitude'].values[which_id]\n",
    "    delta = df['Delta'].values[which_id]\n",
    "    \n",
    "    east = df['East'].values[which_id]\n",
    "    north = df['North'].values[which_id]\n",
    "    \n",
    "    \n",
    "    ref_image_shape = reference_image.shape\n",
    "    query_image_shape = query_image.shape\n",
    "    # convert the images to correct format to be processed by the network: torch Tensors, format B, C, H, W. \n",
    "    # pad both images to the same size, to be processed by network\n",
    "    query_image_, reference_image_ = pad_to_same_shape(query_image, reference_image)\n",
    "\n",
    "    # convert numpy to torch tensor and put it in right format\n",
    "    query_image_ = torch.from_numpy(query_image_).permute(2, 0, 1).unsqueeze(0)\n",
    "    reference_image_ = torch.from_numpy(reference_image_).permute(2, 0, 1).unsqueeze(0)\n",
    "\n",
    "    if estimate_uncertainty:\n",
    "        estimated_flow, uncertainty_components = network.estimate_flow_and_confidence_map(query_image_, reference_image_)\n",
    "    else:\n",
    "        if args.flipping_condition and 'GLUNet' in args.model:\n",
    "            estimated_flow = network.estimate_flow_with_flipping_condition(query_image_, reference_image_,\n",
    "                                                                           mode='channel_first')\n",
    "        else:\n",
    "            estimated_flow = network.estimate_flow(query_image_, reference_image_, mode='channel_first')\n",
    "    # removes the padding\n",
    "    estimated_flow = estimated_flow[:, :, :ref_image_shape[0], :ref_image_shape[1]]\n",
    "\n",
    "    # convert to numpy and reformat\n",
    "    estimated_flow_numpy = estimated_flow.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    # warp the query image according to the estimated flow\n",
    "    warped_query_image = remap_using_flow_fields(query_image, estimated_flow_numpy[:, :, 0],\n",
    "                                                 estimated_flow_numpy[:, :, 1]).astype(np.uint8)\n",
    "\n",
    "    alpha = 0.5\n",
    "    img_warped_overlay_on_target_masked = warped_query_image * alpha + reference_image * alpha\n",
    "    if PLOT:\n",
    "        plt.imshow(img_warped_overlay_on_target_masked.astype(np.uint8))\n",
    "        plt.title('Warped query overlaid reference image')\n",
    "        plt.show()\n",
    "    # confidence estimation + visualization\n",
    "    if not estimate_uncertainty: \n",
    "        raise ValueError\n",
    "    if 'p_r' in uncertainty_components:\n",
    "        uncertainty_key = 'p_r'  # 'inv_cyclic_consistency_error' \n",
    "    else:\n",
    "        uncertainty_key = 'inv_cyclic_consistency_error'\n",
    "    #'p_r', 'inv_cyclic_consistency_error' can also be used as a confidence measure\n",
    "    # 'cyclic_consistency_error' can also be used, but that's an uncertainty measure\n",
    "    min_confidence = 0.30\n",
    "    confidence_map = uncertainty_components[uncertainty_key]\n",
    "    confidence_map = confidence_map[:, :, :ref_image_shape[0], :ref_image_shape[1]]\n",
    "    confidence_map_numpy = confidence_map.squeeze().detach().cpu().numpy()\n",
    "\n",
    "\n",
    "    color = [255, 102, 51]\n",
    "    confidence_map_numpy = confidence_map.squeeze().detach().cpu().numpy()\n",
    "    confident_mask = (confidence_map_numpy > min_confidence).astype(np.uint8)\n",
    "    confident_warped = overlay_semantic_mask(warped_query_image, ann=255 - confident_mask*255, color=color)\n",
    "\n",
    "    # get the mask according to uncertainty estimation\n",
    "    mask_type = 'proba_interval_1_above_10' # 'cyclic_consistency_error_below_2' \n",
    "\n",
    "    mask_padded = estimate_mask(mask_type, uncertainty_components) \n",
    "    if 'warping_mask' in list(uncertainty_components.keys()):\n",
    "        # get mask from internal multi stage alignment, if it took place\n",
    "        mask_padded = uncertainty_components['warping_mask'] * mask_padded\n",
    "\n",
    "    # remove the padding\n",
    "    mask = mask_padded[:, :ref_image_shape[0], :ref_image_shape[1]]\n",
    "\n",
    "    # remove point that lead to outside the query image\n",
    "    mapping_estimated = convert_flow_to_mapping(estimated_flow)\n",
    "    mask = mask & mapping_estimated[:, 0].ge(0) & mapping_estimated[:, 1].ge(0) & \\\n",
    "    mapping_estimated[:, 0].le(query_image_shape[1] - 1) & mapping_estimated[:, 1].le(query_image_shape[0] - 1)\n",
    "\n",
    "    mkpts_query, mkpts_ref = matches_from_flow(estimated_flow, mask)\n",
    "\n",
    "    confidence_values = confidence_map.squeeze()[mask.squeeze()].cpu().numpy()\n",
    "    sort_index = np.argsort(np.array(confidence_values)).tolist()[::-1]  # from highest to smallest\n",
    "    confidence_values = np.array(confidence_values)[sort_index]\n",
    "    mkpts_query = np.array(mkpts_query)[sort_index]\n",
    "    mkpts_ref = np.array(mkpts_ref)[sort_index]\n",
    "\n",
    "    if len(mkpts_query) > 5:\n",
    "        M, mask = cv.findHomography(mkpts_query, mkpts_ref)\n",
    "    else:\n",
    "        M = np.zeros((3,3))\n",
    "    confidence_map_numpy = confidence_map_numpy.reshape(-1)\n",
    "    res = np.zeros(36)\n",
    "    res[:9] = M.reshape(-1)\n",
    "    res[9:20] =  np.quantile(estimated_flow_numpy[:,:,0],[0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1])\n",
    "    res[20:31] =  np.quantile(estimated_flow_numpy[:,:,1],[0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1])\n",
    "    res[31:36] = np.quantile(confidence_map_numpy,[0,0.1,0.25,0.5,1])\n",
    "    \n",
    "    L.append(res[None,:])\n",
    "\"\"\" Time to concatenate the list of rows into a 2D array. \"\"\"\n",
    "X = np.concatenate(L,axis=0)\n",
    "# Add appropriate column names\n",
    "col = [f'mat_{i}' for i in range(9)] + [f'ef0_{i}' for i in range(11)]  + [f'ef1_{i}' for i in range(11)] + [f'conf_{i}' for i in range(5)]\n",
    "# Create feature dataframe using the 2d array as data.\n",
    "feat_df = pd.DataFrame(X,index=range(X.shape[0]),columns=col)\n",
    "# Save the feature dataframe\n",
    "os.makedirs(osp.join(DS_FOLDER,'features',model),exist_ok=True)\n",
    "display(feat_df)\n",
    "\n",
    "feat_df.loc[~pd.isna(df['North']), :].to_csv(osp.join(DS_FOLDER,'features',model, 'trainval.csv'),index=False)\n",
    "feat_df.loc[pd.isna(df['North']), :].to_csv(osp.join(DS_FOLDER,'features',model, 'test.csv'),index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
